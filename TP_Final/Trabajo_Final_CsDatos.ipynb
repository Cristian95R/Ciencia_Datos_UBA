{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Scrapping de Supermercados**"
      ],
      "metadata": {
        "id": "gA-huymTGQjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstract\n",
        "El acceso a alimentos básicos a precios accesibles es un desafío constante para las familias, especialmente en un contexto donde el pais sufre de cambios economicos bruscos. En la vida cotidiana, decidir dónde realizar las compras se vuelve una cuestión clave, ya que la variación de precios entre diferentes supermercados y regiones puede impactar significativamente en el poder adquisitivo. Este informe se centra en el análisis de los precios de los principales alimentos utilizados en el desayuno argentino, un momento fundamental en la alimentación diaria, considerando la importancia social y cultural de este hábito.\n",
        "En la actualidad, la proliferación de plataformas de comercio electrónico permite comparar precios de manera más eficiente. Es por eso que decidimos realizar una pequeña comparacion entre los supermercados de la ciudad y los que se encuentran en basto territorio de la provincia de Buenos Aires.\n",
        "La idea es ver si existe diferencias notables de precios y productos en ambas regiones y como eso puede afectar al ciudadano de estas.\n",
        "\n",
        "Los resultados de este estudio permitirán comprender mejor la dinámica de precios en la región geografica de Buenos Aires y ofrecerán un insumo valioso para el diseño de estrategias de consumo más eficientes en un contexto económico desafiante"
      ],
      "metadata": {
        "id": "rxNKa5CIGQX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Objetivo\n",
        "El objetivo principal de este análisis es identificar si existen diferencias notables en los precios y la variedad de productos destinados al desayuno argentino entre ambas regiones y analizar cómo estas diferencias pueden afectar a los ciudadanos de cada área. Se busca, así, aportar datos relevantes que permitan visibilizar posibles desigualdades y brindar herramientas para que los consumidores puedan tomar decisiones informadas a la hora de realizar sus compras."
      ],
      "metadata": {
        "id": "3FHe2CA9I1O9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preguntas de Interés\n",
        "\n",
        "*   ¿Existen diferencias entre los productos que se venden en los supermercados? ¿Y entre zonas?\n",
        "*   ¿Existen diferencias de precios entre zonas o entre supermercados?\n",
        "*   ¿Donde podría adquirir los productos abonando lo menos posible?\n",
        "*   ¿Cual de estos supermercados posee los productos mas exclusivos?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uLIwb7PMJQmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Método Utilizado\n",
        "La recolección de datos se realizará sobre cinco supermercados: tres ubicados en la Ciudad de Buenos Aires (COTO, Carrefour y Disco) y dos en distintas localidades del interior de la provincia de Buenos Aires (La Anónima y La Cooperativa Obrera).\n",
        "\n",
        "El método consiste en scrapear los sitios web de cada supermercado utilizando Python, con la librería Selenium. Para el desarrollo y la ejecución de los scripts se emplearán Visual Studio Code y Jupyter Notebook.\n",
        "\n",
        "Los productos seleccionados para el análisis serán: Manteca, Pan, Café, Té, Mate, Yogur, Mermelada y Queso Crema.\n",
        "\n",
        "Una vez recolectados los datos, se realizará un proceso de limpieza y transformación para adecuarlos al análisis, utilizando R. Finalmente, la visualización de los resultados se llevará a cabo en Looker Studio.\n",
        "\n",
        "queriamos concretar el uso de todas las herramientass que aprendimos en la materia"
      ],
      "metadata": {
        "id": "kvWQVxw-Jmv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Adquisition\n",
        "Se procede a mostrar las querys que sirvieron para scrapear cada supermercado. Cada uno de ellos tiene su particularidad debido a que las paginas son diferentes. Por lo cual necesito armar un scrapping individual para cada caso.\n",
        "Para realizar el scrapping de la forma en como lo realicé se deben tener instalados en la computadora los siguientes programas:\n",
        "\n",
        "\n",
        "*   Navegador Firefox con el GekkoDriver\n",
        "*   Visual Studio Code con la extencion de Jupyter Notebook\n",
        "*   Jupyter Notebook\n",
        "*   Python"
      ],
      "metadata": {
        "id": "Mo7J4WAbJ5RY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Librerias a Utilizar en el scrapping\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.firefox.service import Service\n",
        "from selenium.webdriver.firefox.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "from unidecode import unidecode"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WdWsHFsRJ48y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estas librerías permiten que Selenium pueda abrir un navegador, buscar elementos de diferentes maneras, esperar que se carguen, y tener control avanzado para hacer scrapping o automatizar tareas en la web.\n",
        "\n",
        "*   webdriver: Importa el núcleo de Selenium para poder abrir y controlar navegadores web en nuestro caso utilizo firefox. De esta libreria se desprenden varias librerias que nos ayudan a configurar la busqueda correspondiente.\n",
        "*   By: sirve para decirle a Selenium cómo va a buscar un elemento en la página (por ID, por clase, por CSS selector, etc).\n",
        "*   Service: es el puente entre python y firefox, en nuestro caso indica la ruta del ejecutable de GeckoDriver\n",
        "*   Options: Sirve para configurar las opciones especiales del navegador Firefox\n",
        "*   WebDriverWait: Sirve para esperar a que suceda algo en la página (por ejemplo, que cargue un botón antes de hacer click). Es útil para evitar errores por cargar muy rápido.\n",
        "*   expected_conditions as EC: Se usa junto a web driver para definir las condiciones de espera.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "azAmflsTLQ0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scrapping de Disco**"
      ],
      "metadata": {
        "id": "L_O-oQV1N7vR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj2OmoitFSzp"
      },
      "outputs": [],
      "source": [
        "# Configuración para Firefox\n",
        "service = Service(executable_path=r\"C:\\tools\\geckodriver\\geckodriver.exe\")\n",
        "firefox_options = Options()\n",
        "firefox_options.binary_location = r\"C:\\Program Files\\Mozilla Firefox\\firefox.exe\"\n",
        "driver = webdriver.Firefox(service=service, options=firefox_options)\n",
        "\n",
        "# Lista de productos para buscar\n",
        "productos_a_buscar = [\n",
        "    {\"nombre\": \"cafe\", \"base_url\": \"https://www.disco.com.ar/cafe?_q=cafe&map=ft&page=\"},\n",
        "    {\"nombre\": \"te\", \"base_url\": \"https://www.disco.com.ar/te?_q=te&map=ft&page=\"},\n",
        "    {\"nombre\": \"mate\", \"base_url\": \"https://www.disco.com.ar/mate?_q=mate&map=ft&page=\"},\n",
        "    {\"nombre\": \"pan\", \"base_url\": \"https://www.disco.com.ar/pan?_q=pan&map=ft&page=\"},\n",
        "    {\"nombre\": \"yogur\", \"base_url\": \"https://www.disco.com.ar/yogur?_q=yogur&map=ft&page=\"},\n",
        "    {\"nombre\": \"mermelada\", \"base_url\": \"https://www.disco.com.ar/mermelada?_q=mermelada&map=ft&page=\"},\n",
        "    {\"nombre\": \"queso\", \"base_url\": \"https://www.disco.com.ar/queso%20crema?_q=queso%20crema&map=ft&page=\"},\n",
        "    {\"nombre\": \"manteca\", \"base_url\": \"https://www.disco.com.ar/manteca?_q=manteca&map=ft&page=\"},\n",
        "]\n",
        "\n",
        "resultados = []\n",
        "for prod in productos_a_buscar:\n",
        "    pagina = 1\n",
        "    while True:\n",
        "        url = prod[\"base_url\"] + str(pagina)\n",
        "        driver.get(url)\n",
        "        print(f\"\\n== {prod['nombre']} | Página {pagina} ({url}) ==\")\n",
        "        time.sleep(5)\n",
        "\n",
        "        # Aceptar cookies (solo en la primera página)\n",
        "        if pagina == 1:\n",
        "            try:\n",
        "                btn_cookies = driver.find_element(By.CSS_SELECTOR, 'button#onetrust-accept-btn-handler')\n",
        "                btn_cookies.click()\n",
        "                print(\"Cookies aceptadas.\")\n",
        "                time.sleep(1)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        productos = []\n",
        "        for _ in range(10):\n",
        "            productos = driver.find_elements(By.CSS_SELECTOR, \"article.vtex-product-summary-2-x-element\")\n",
        "            if productos:\n",
        "                break\n",
        "            time.sleep(2)\n",
        "\n",
        "        if not productos:\n",
        "            print(\"No se encontraron productos en la página. Fin para\", prod['nombre'])\n",
        "            break\n",
        "\n",
        "        for art in productos:\n",
        "            try:\n",
        "                nombre = art.find_element(By.CSS_SELECTOR, 'span.vtex-product-summary-2-x-productBrand').text\n",
        "            except:\n",
        "                nombre = \"N/A\"\n",
        "            try:\n",
        "                # Cambiado el selector para Disco (precio)\n",
        "                precio = art.find_element(By.CSS_SELECTOR, \"div[id^='priceContainer']\").text\n",
        "            except:\n",
        "                precio = \"N/A\"\n",
        "            resultados.append({\n",
        "                \"Producto\": nombre,\n",
        "                \"Precio\": precio,\n",
        "                \"Página\": pagina,\n",
        "                \"Busqueda\": prod[\"nombre\"]\n",
        "            })\n",
        "        print(f\"Página {pagina}: {len(productos)} productos extraídos.\")\n",
        "\n",
        "        # Intentar ir a la siguiente página cambiando la URL (incrementa hasta que no encuentre productos)\n",
        "        pagina += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "driver.quit()\n",
        "\n",
        "# Crear DataFrame y guardar resultados en un excel\n",
        "disco_df = pd.DataFrame(resultados)\n",
        "disco_df.to_excel(r'C:\\Users\\Cristian\\Documents\\productos_Disco.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scrapping de Carrefour**\n",
        "Al scrapear Carrefour me di cuenta que tenia una estructura parecida a la pagina de Disco por lo que fue copiar el modelo de scrapping anterior."
      ],
      "metadata": {
        "id": "DxyzGFoXPb0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración para Firefox (ajusta las rutas si hace falta)\n",
        "service = Service(executable_path=r\"C:\\tools\\geckodriver\\geckodriver.exe\")\n",
        "firefox_options.binary_location = r\"C:\\Program Files\\Mozilla Firefox\\firefox.exe\"\n",
        "\n",
        "# Lista de productos a buscar con su URL base\n",
        "productos = [\n",
        "    {\"nombre_busqueda\": \"cafe\", \"url\": \"https://www.carrefour.com.ar/Desayuno-y-merienda/Cafe?order=\"},\n",
        "    {\"nombre_busqueda\": \"te\", \"url\": \"https://www.carrefour.com.ar/Desayuno-y-merienda/Te-e-Infusiones?order=\"},\n",
        "    {\"nombre_busqueda\": \"mate\", \"url\": \"https://www.carrefour.com.ar/mate?_q=mate&map=ft\"},\n",
        "    {\"nombre_busqueda\": \"pan\", \"url\": \"https://www.carrefour.com.ar/Panaderia/Panificados?order=\"},\n",
        "    {\"nombre_busqueda\": \"yogur\", \"url\": \"https://www.carrefour.com.ar/Lacteos-y-productos-frescos/Yogures?order=\"},\n",
        "    {\"nombre_busqueda\": \"mermelada\", \"url\": \"https://www.carrefour.com.ar/Desayuno-y-merienda/Mermeladas-y-otros-dulces/Mermeladas-dulces-y-jaleas?order=\"},\n",
        "    {\"nombre_busqueda\": \"queso\", \"url\": \"https://www.carrefour.com.ar/Lacteos-y-productos-frescos/Quesos/Quesos-cremas-y-untables?order=\"},\n",
        "    {\"nombre_busqueda\": \"manteca\", \"url\": \"https://www.carrefour.com.ar/lacteos-y-productos-frescos?map=category-1,tipo-de-producto&order=&query=/lacteos-y-productos-frescos/manteca&searchState\"},\n",
        "\n",
        "]\n",
        "\n",
        "options = webdriver.FirefoxOptions()\n",
        "options.add_argument('--width=1200')\n",
        "options.add_argument('--height=900')\n",
        "driver = webdriver.Firefox(options=options)\n",
        "wait = WebDriverWait(driver, 35)\n",
        "\n",
        "resultados = []\n",
        "\n",
        "for producto in productos:\n",
        "    nombre_producto = producto[\"nombre_busqueda\"]\n",
        "    base_url = producto[\"url\"]\n",
        "    pagina = 1\n",
        "    while True:\n",
        "        url = base_url + f\"&page={pagina}\"\n",
        "        driver.get(url)\n",
        "        print(f\"\\n== Buscando: {nombre_producto} - Página {pagina} ({url}) ==\")\n",
        "\n",
        "        # Sólo aceptar cookies la primera vez\n",
        "        if pagina == 1 and producto == productos[0]:\n",
        "            try:\n",
        "                btn_cookies = wait.until(\n",
        "                    EC.element_to_be_clickable((By.CSS_SELECTOR, 'button#onetrust-accept-btn-handler'))\n",
        "                )\n",
        "                btn_cookies.click()\n",
        "                print(\"Cookies aceptadas.\")\n",
        "                time.sleep(2)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Esperar productos\n",
        "        productos_encontrados = []\n",
        "        for _ in range(10):\n",
        "            productos_encontrados = driver.find_elements(By.CSS_SELECTOR, \"article.vtex-product-summary-2-x-element\")\n",
        "            if productos_encontrados:\n",
        "                break\n",
        "            time.sleep(2)\n",
        "\n",
        "        if not productos_encontrados:\n",
        "            print(\"No se encontraron productos en la página. Fin de la búsqueda de\", nombre_producto)\n",
        "            break\n",
        "\n",
        "        for prod in productos_encontrados:\n",
        "            try:\n",
        "                nombre = prod.find_element(By.CSS_SELECTOR, 'span.vtex-product-summary-2-x-productBrand').text\n",
        "            except:\n",
        "                nombre = \"N/A\"\n",
        "            try:\n",
        "                precio = prod.find_element(By.CSS_SELECTOR, 'span.valtech-carrefourar-product-price-0-x-sellingPriceValue').text\n",
        "            except:\n",
        "                precio = \"N/A\"\n",
        "            resultados.append({\n",
        "                \"busqueda\": nombre_producto,\n",
        "                \"nombre\": nombre,\n",
        "                \"precio\": precio\n",
        "            })\n",
        "\n",
        "        pagina += 1\n",
        "        time.sleep(2)\n",
        "\n",
        "driver.quit()\n",
        "\n",
        "carrefour_DF = pd.DataFrame(resultados)\n",
        "print(carrefour_DF.head())\n",
        "carrefour_DF.to_excel(r'C:\\Users\\Cristian\\Documents\\carrefour_productos.xlsx', index=False)"
      ],
      "metadata": {
        "id": "i0XTnuxOPp-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#El archivo que generaba el scrapping es el siguiente\n",
        "import pandas as pd\n",
        "carrefour_DF = pd.read_excel('carrefour_productos.xlsx')\n",
        "print(carrefour_DF.head())\n"
      ],
      "metadata": {
        "id": "x1NmQZe2RArG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scrappig de COTO**\n",
        "Se modifica un poco el esquema de busqueda debido a que la estructura de la pagina es diferente a las anteriores.\n"
      ],
      "metadata": {
        "id": "OyV0regGRdHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service = Service(executable_path=r\"C:\\tools\\geckodriver\\geckodriver.exe\")\n",
        "firefox_options = Options()\n",
        "firefox_options.binary_location = r\"C:\\Program Files\\Mozilla Firefox\\firefox.exe\"\n",
        "\n",
        "driver = webdriver.Firefox(service=service, options=firefox_options)\n",
        "\n",
        "# Lista de productos y URLs de Coto Digital (agregá los que quieras)\n",
        "productos_a_buscar = [\n",
        "    {\"nombre\": \"cafe\", \"url\": \"https://www.cotodigital.com.ar/sitios/cdigi/categoria?_dyncharset=utf-8&Dy=1&Ntt=cafe\"},\n",
        "    {\"nombre\": \"te\", \"url\": \"https://www.cotodigital.com.ar/sitios/cdigi/categoria?_dyncharset=utf-8&Dy=1&Ntt=te\"},\n",
        "    {\"nombre\": \"mate\", \"url\": \"https://www.cotodigital.com.ar/sitios/cdigi/categoria?_dyncharset=utf-8&Dy=1&Ntt=mate\"},\n",
        "    {\"nombre\": \"pan\", \"url\": \"https://www.cotodigital.com.ar/sitios/cdigi/categoria?_dyncharset=utf-8&Dy=1&Ntt=pan\"},\n",
        "    {\"nombre\": \"yogur\", \"url\": \"https://www.cotodigital.com.ar/sitios/cdigi/sitios/cdigi/categoria/catalogo-frescos-l%C3%A1cteos-yogures/_/N-rfedtp%3FNf%3Dproduct.endDate%257CGTEQ%2B1.7400096E12%257C%257Cproduct.startDate%257CLTEQ%2B1.7400096E12&Nr%3DAND%2528product.language%253Aespa%25C3%25B1ol%252Cproduct.sDisp_200%253A1004%252COR%2528product.siteId%253ACotoDigital%2529%2529&Ns%3Dproduct.TOTALDEVENTAS%257C1&format%3Djson\"},\n",
        "    {\"nombre\": \"mermelada\", \"url\": \"https://www.cotodigital.com.ar/sitios/cdigi/categoria?_dyncharset=utf-8&Dy=1&Ntt=mermelada\"},\n",
        "    {\"nombre\": \"queso\", \"url\": \"https://www.cotodigital.com.ar/sitios/cdigi/categoria?_dyncharset=utf-8&Dy=1&Ntt=queso%20crema\"},\n",
        "    {\"nombre\": \"manteca\", \"url\": \"https://www.cotodigital.com.ar/sitios/cdigi/categoria?_dyncharset=utf-8&Dy=1&Ntt=manteca\"}\n",
        "]\n",
        "\n",
        "resultados = []\n",
        "\n",
        "for prod in productos_a_buscar:\n",
        "    driver.get(prod[\"url\"])\n",
        "    time.sleep(8)\n",
        "    pagina = 1\n",
        "    while True:\n",
        "        time.sleep(2)\n",
        "        try:\n",
        "            aside = driver.find_element(By.CSS_SELECTOR, \"aside.mt-3\")\n",
        "        except:\n",
        "            print(f\"No se encontró el contenedor principal en la página {pagina} de {prod['nombre']}.\")\n",
        "            break\n",
        "\n",
        "        # Scrappear productos de la página actual\n",
        "        cards = aside.find_elements(By.CSS_SELECTOR, \"div.producto-card\")\n",
        "        for card in cards:\n",
        "            try:\n",
        "                nombre = card.find_element(By.CSS_SELECTOR, \"h3.nombre-producto\").text\n",
        "            except:\n",
        "                nombre = \"N/A\"\n",
        "            try:\n",
        "                precio = card.find_element(By.CSS_SELECTOR, \"h4.card-title\").text\n",
        "            except:\n",
        "                precio = \"N/A\"\n",
        "            resultados.append({\n",
        "                \"Producto\": nombre,\n",
        "                \"Precio\": precio,\n",
        "                \"Página\": pagina,\n",
        "                \"Busqueda\": prod[\"nombre\"]\n",
        "            })\n",
        "\n",
        "        print(f\"{prod['nombre']} | Página {pagina}: {len(cards)} productos extraídos.\")\n",
        "\n",
        "        # Buscar el paginador y el botón \"Siguiente\"\n",
        "        try:\n",
        "            paginador = aside.find_element(By.CSS_SELECTOR, \"ul.pagination\")\n",
        "            siguiente = paginador.find_element(By.XPATH, \".//a[contains(text(), 'Siguiente')]\")\n",
        "            if \"disabled\" in siguiente.get_attribute(\"class\"):\n",
        "                print(f\"Fin de las páginas para {prod['nombre']}.\")\n",
        "                break\n",
        "            driver.execute_script(\"arguments[0].scrollIntoView();\", siguiente)\n",
        "            time.sleep(1)\n",
        "            siguiente.click()\n",
        "            pagina += 1\n",
        "            time.sleep(4)\n",
        "        except Exception as e:\n",
        "            print(f\"No se encontró el botón Siguiente o error para {prod['nombre']}: {e}\")\n",
        "            break\n",
        "\n",
        "driver.quit()\n",
        "\n",
        "# Guardar DataFrame y exportar a excel la  informacion\n",
        "coto_df = pd.DataFrame(resultados)\n",
        "coto_df.to_excel(r'C:\\Users\\Cristian\\Documents\\coto_productos.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "NcQh14teRiEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#El archivo que generaba el scrapping es el siguiente\n",
        "import pandas as pd\n",
        "coto_DF = pd.read_excel('coto_productos.xlsx')\n",
        "print(coto_DF.head())"
      ],
      "metadata": {
        "id": "7w6VrgIASnKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scrapping de la Cooperativa Obrera**"
      ],
      "metadata": {
        "id": "4xPOmgabSiiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración para Firefox\n",
        "service = Service(executable_path=r\"C:\\tools\\geckodriver\\geckodriver.exe\")\n",
        "firefox_options = Options()\n",
        "firefox_options.binary_location = r\"C:\\Program Files\\Mozilla Firefox\\firefox.exe\"\n",
        "driver = webdriver.Firefox(service=service, options=firefox_options)\n",
        "\n",
        "productos_a_buscar = [\n",
        "    \"cafe\", \"te\", \"mate\", \"pan\", \"yogur\", \"mermelada\", \"queso crema\", \"manteca\"\n",
        "]\n",
        "\n",
        "resultados = []\n",
        "\n",
        "for producto in productos_a_buscar:\n",
        "    pagina = 1\n",
        "    while True:\n",
        "        # Construcción de URL: ?busqueda=PRODUCTO&p=1\n",
        "        url = f\"https://www.lacoopeencasa.coop/listado/busqueda-avanzada/{producto}?p={pagina}\"\n",
        "        driver.get(url)\n",
        "        print(f\"\\n== Producto: {producto} | Página {pagina} ({url}) ==\")\n",
        "        time.sleep(15)\n",
        "\n",
        "        # Scroll humano\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/2);\")\n",
        "        time.sleep(1.5)\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(1.5)\n",
        "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
        "        time.sleep(1.5)\n",
        "\n",
        "        # Buscar productos\n",
        "        productos = driver.find_elements(By.CSS_SELECTOR, \"div.card-content\")\n",
        "        if not productos:\n",
        "            print(f\"No se encontraron productos para {producto} en página {pagina}. Paso al siguiente producto.\")\n",
        "            break\n",
        "\n",
        "        for prod in productos:\n",
        "            # Nombre\n",
        "            try:\n",
        "                nombre = prod.find_element(By.CSS_SELECTOR, 'div.card-descripcion').text\n",
        "            except:\n",
        "                nombre = \"N/A\"\n",
        "            # Precio entero y decimal (el precio está dividido)\n",
        "            try:\n",
        "                precio_entero = prod.find_element(By.CSS_SELECTOR, 'div.precio-entero').text\n",
        "            except:\n",
        "                precio_entero = \"\"\n",
        "            try:\n",
        "                precio_decimal = prod.find_element(By.CSS_SELECTOR, 'div.precio-decimal').text\n",
        "            except:\n",
        "                precio_decimal = \"\"\n",
        "            precio = f\"${precio_entero},{precio_decimal}\" if precio_entero else \"N/A\"\n",
        "\n",
        "            resultados.append({\n",
        "                \"Producto\": nombre,\n",
        "                \"Precio\": precio,\n",
        "                \"Página\": pagina,\n",
        "                \"Busqueda\": producto\n",
        "            })\n",
        "\n",
        "        print(f\"Página {pagina}: {len(productos)} productos extraídos.\")\n",
        "\n",
        "        # Si no hay un botón \">\" habilitado, cortá el bucle\n",
        "        try:\n",
        "            btn_sig = driver.find_element(By.CSS_SELECTOR, 'li.pagination-next:not(.disabled) a')\n",
        "            pagina += 1\n",
        "        except:\n",
        "            break\n",
        "\n",
        "        time.sleep(12)\n",
        "\n",
        "driver.quit()\n",
        "\n",
        "# Guardar resultados en Excel\n",
        "# Admeas le creo una columna con el nombre del supermercado\n",
        "coope_df = pd.DataFrame(resultados)\n",
        "coope_df[\"Supermercado\"] = \"Cooperativa Obrera\"\n",
        "coope_df.to_excel(r'C:\\Users\\Cristian\\Documents\\productos_coope.xlsx', index=False)"
      ],
      "metadata": {
        "id": "Iy6smsbTS2em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#El archivo que generaba el scrapping es el siguiente\n",
        "coope_DF = pd.read_excel('productos_LaCoope.xlsx')\n",
        "print(coope_DF.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "187fcc19-b31b-4fe6-a193-0d4dedc0dadb",
        "id": "sUYDLEH8UDeC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Producto      Precio Busqueda  \\\n",
            "0  CafE Torrado InstantANeo La Virginia ClASico F...  $$5.990,00     cafe   \n",
            "1  CafE Torrado InstantANeo La Virginia Suave Fra...  $$5.990,00     cafe   \n",
            "2  Cafe InstantANeo Nescafe TradiciON Doypack 100grs  $$6.989,00     cafe   \n",
            "3  CafE Torrado Molido Morenita Intenso En Saquit...  $$4.851,00     cafe   \n",
            "4  CafE Torrado Molido La Virginia Equilibrado En...  $$5.026,00     cafe   \n",
            "\n",
            "         Supermercado  \n",
            "0  Cooperativa Obrera  \n",
            "1  Cooperativa Obrera  \n",
            "2  Cooperativa Obrera  \n",
            "3  Cooperativa Obrera  \n",
            "4  Cooperativa Obrera  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scrapping de La Anonima**\n",
        "El nivel de scrapping en esta pagina se pone un poco mas complicado debido a que la pagina bloquea a los robots que acceden a ella. Una vez dentro de la misma logre tomar la informacion de 3 productos antes de que se bloquee y muestre la pagina en blanco o lance un error. Se logra realizar la tarea dividiendo el scrapping cada 3 productos y que luego cuando se bloquee guarde lo que se logro hasta ese momento en un excel. Luego espero un tiempo y vuelvo a ejecutar el scrib, primero el mismo consulta la base y si no encuentra el producto empieza a scrapear por el primer producto que falte. Para que no me detecte tan rapido le cree una breve simulacion humana y le indique tiempos de espera por pagina mas largos ya sea para que cargue toda la pagina y ademas que no lo detecte como una actividad de un robot."
      ],
      "metadata": {
        "id": "ZvBUMh7mUTp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "archivo_salida = \"productos_anonima.xlsx\"\n",
        "\n",
        "# Lee el Excel existente si hay, sino crea lista vacía\n",
        "if os.path.exists(archivo_salida):\n",
        "    anonima_df = pd.read_excel(archivo_salida)\n",
        "    resultados = anonima_df.to_dict(orient='records')\n",
        "    print(f\"Archivo existente: {len(anonima_df)} filas previas.\")\n",
        "else:\n",
        "    resultados = []\n",
        "\n",
        "# Configuración Selenium\n",
        "service = Service(executable_path=r\"C:\\tools\\geckodriver\\geckodriver.exe\")\n",
        "firefox_options = Options()\n",
        "firefox_options.binary_location = r\"C:\\Program Files\\Mozilla Firefox\\firefox.exe\"\n",
        "driver = webdriver.Firefox(service=service, options=firefox_options)\n",
        "\n",
        "#  Lista de productos a buscar (tanda)\n",
        "productos_a_buscar = [ \"cafe\", \"te\", \"mate\", \"pan\", \"yogur\", \"mermelada\", \"queso+crema+\", \"manteca\"]\n",
        "\n",
        "# Detecta cuáles productos faltan (no scrapeados antes)\n",
        "productos_hechos = set([r[\"Busqueda\"] for r in resultados])\n",
        "productos_a_scrapear = [p for p in productos_a_buscar if p not in productos_hechos]\n",
        "\n",
        "# Cambiá el número para la tanda deseada\n",
        "TANDA = 3  # cuántos productos scrapeás por tanda\n",
        "\n",
        "for producto in productos_a_scrapear[:TANDA]:\n",
        "    pagina = 1\n",
        "    while True:\n",
        "        url = f\"https://supermercado.laanonimaonline.com/buscar?pag={pagina}&clave={producto}\"\n",
        "        driver.get(url)\n",
        "        print(f\"\\n== Producto: {producto} | Página {pagina} ({url}) ==\")\n",
        "\n",
        "        # Scroll humano\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/2);\")\n",
        "        time.sleep(random.uniform(1.5, 5.5))\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(random.uniform(1.5, 5.5))\n",
        "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
        "        time.sleep(random.uniform(1.5, 5.5))\n",
        "\n",
        "        # Intentos para cargar productos\n",
        "        intentos = 0\n",
        "        productos = []\n",
        "        while not productos and intentos < 8:\n",
        "            productos = driver.find_elements(By.CSS_SELECTOR, \"div[class*='producto item']\")\n",
        "            if productos:\n",
        "                break\n",
        "            time.sleep(random.uniform(6, 15))\n",
        "            intentos += 1\n",
        "\n",
        "        if not productos:\n",
        "            print(f\"No hay productos para '{producto}' en página {pagina} (o la página no carga). Paso al próximo producto.\")\n",
        "            break\n",
        "\n",
        "        for prod in productos:\n",
        "            # Nombre del producto\n",
        "            try:\n",
        "                nombre = prod.find_element(By.CSS_SELECTOR, \"a[id^='btn_nombre_imetrics_']\").text.strip()\n",
        "            except:\n",
        "                nombre = \"N/A\"\n",
        "            # Precio del producto\n",
        "            try:\n",
        "                precio = prod.find_element(By.CSS_SELECTOR, \"div.precio-promo\").text.strip()\n",
        "            except:\n",
        "                precio = \"N/A\"\n",
        "            resultados.append({\n",
        "                \"Producto\": nombre,\n",
        "                \"Precio\": precio,\n",
        "                \"Página\": pagina,\n",
        "                \"Busqueda\": producto\n",
        "            })\n",
        "\n",
        "        print(f\"Página {pagina}: {len(productos)} productos extraídos.\")\n",
        "        pagina += 1\n",
        "        time.sleep(random.uniform(8, 18))\n",
        "\n",
        "    # **Guarda el archivo después de cada producto**\n",
        "    # Ademas le creo una columna con el nombre del supermercado\n",
        "    anonima_df[\"Supermercado\"] = \"La Anónima\"\n",
        "    anonima_df = pd.DataFrame(resultados)\n",
        "    anonima_df.drop_duplicates(subset=[\"Producto\", \"Precio\", \"Busqueda\"], inplace=True)\n",
        "    anonima_df.to_excel(archivo_salida, index=False)\n",
        "    print(f\"Guardado: {len(anonima_df)} filas hasta ahora.\")\n",
        "\n",
        "driver.quit()\n",
        "#Para Exportar al excel\n",
        "anonima_df.to_excel(r'C:\\Users\\Cristian\\Documents\\productos_LaAnonima.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "8oZi4Z6IXwyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#El archivo que generaba el scrapping es el siguiente\n",
        "Anonima_DF = pd.read_excel('productos_LaAnonima.xlsx')\n",
        "print(Anonima_DF.head())"
      ],
      "metadata": {
        "id": "HIWUeAGIZq3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¿Que es lo que queda luego del scrapping?**\n",
        "En nuestro caso para cada supermercado se formó una tabla conformada por las siguientes columnas:\n",
        "\n",
        "Producto: Contiene toda la informacion acerca del producto (nombre, marca y cantidad/peso) Tipo: String\n",
        "\n",
        "Precio : Precio del producto expresado en pesos y con decimales. Tipo: String\n",
        "\n",
        "Página: Número  de la pagina donde se encontró el producto( sirve para realizar un pequeño control al scraping). Tipo: Integer\n",
        "\n",
        "Búsqueda: Nombre generico  del producto objetivo buscado en el scrapping. Sirve para controlar si se realizo correctamente la busqueda. Tipo: String\n",
        "\n",
        "Supermercado: Nombre del supermercado que fue scrapeado. Tipo: String"
      ],
      "metadata": {
        "id": "8nzcNJUlZYoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Wrangling\n",
        "Una vez obtenido los datos a travez de cada scrapping realizado se procede a limpiar, transformar y organizar datos en bruto para que sean útiles y esten listos para el análisis y la visualización\n",
        "\n",
        "Se realizaran tres acciones principales:\n",
        "\n",
        "*   Transformacion de datos: Al no venir la informacion del todo clara se utilizan diferentes herramientas para transformar los datos y nos sirvan para el analisis. Se crea un nuevo dataframe y un nuevo excel con todos los datos de todos los supermercados.\n",
        "*   Control de Datos: Con el objetivo de analizar los productos obtenidos; se verifica la informacion tomando algunos registros de prueba y comparandolos con el original de la pagina web. En esta parte son necesarias para el analisis las columnas Busqueda y Pagina.\n",
        "*   Eliminacion de Datos: Eliminamos datos innecesarios de productos que no entran en nuestro analisis. Eliminacion de nulos y eliminacion de columnas que ya no interesan para el analisis final.\n",
        "\n"
      ],
      "metadata": {
        "id": "t8ReMaH0cYuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Librerias a utilizar\n",
        "import pandas as pd\n",
        "import re\n",
        "from unidecode import unidecode"
      ],
      "metadata": {
        "id": "zzK7n9STfi7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modificaciones a La Anonima**"
      ],
      "metadata": {
        "id": "xQlB0Lw1e_fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Eliminar Filas Nulas\n",
        "anonima_df = anonima_df.dropna()\n",
        "#Eliminar las filas donde alguna columna tenga \"N/A\"\n",
        "anonima_df_F = anonima_df[~anonima_df.isin([\"N/A\"]).any(axis=1)]\n",
        "#Palabras a excluir del analisis por no pertenecer al mismo\n",
        "palabras_excluir = [\n",
        "    \"Endulzante\", \"Crema\", \"Matero\", \"Bolsa\", \"Rallado\", \"Pancho\", \"Hamburguesas\", \"Viena\", \"Levadura\",\n",
        "    \"Jabon\", \"Harina\", \"Margarina\", \"Untar y Cocinar\", \"Premezcla\", \"Pañal\", \"Rollo\", \"Caldo\",\n",
        "    \"Pañales\", \"Pañuelos\", \"Panceta\", \"Paño\", \"Acondicionador\", \"Panzerottis\", \"Salchichas\",\n",
        "    \"Ropa\", \"Confitura\", \"Chizitos\", \"Papel\",\"Pochoclo\"\n",
        "]\n",
        "# Filtrá el DataFrame eliminando las filas que contengan esas palabras en la columna \"Producto\"\n",
        "#Se une todas las palabras en un mismo registro insencible a tamaño de letra. Lugo se filtra y me quedo solo con las filas que NO contengan esas palabras en la columna producto\n",
        "regex_excluir = \"|\".join([re.escape(palabra)for palabra in palabras_excluir])\n",
        "anonima_df_F = anonima_df_F[~anonima_df_F[\"Producto\"].str.contains(regex_excluir, case=False, na=False)]\n",
        "\n",
        "#Creamos una funcion para crear una columna de gramajes\n",
        "def extraer_gramaje(texto):\n",
        "    patron = r'(\\d+(?:[\\.,]\\d+)?\\s?(?:kg|g|grs?|ml|l|cc|un|kgs|gr|lts?|u))'\n",
        "   # match = re.search(r'(\\d+([.,]\\d+)?\\s?(kg|g|gr|ml|u))', texto.lower())\n",
        "    resultado = re.findall(patron,texto.lower())\n",
        "    return resultado[0] if resultado else \"\"\n",
        "\n",
        "#Aplico la funcion a la columna Producto\n",
        "anonima_df_F[\"Gramaje\"] = anonima_df_F[\"Producto\"].apply(extraer_gramaje)\n",
        "\n",
        "#Creo una funcion para limpiar la columna producto.\n",
        "def quitar_gramaje(texto, gramaje):\n",
        "    if pd.isna(gramaje) or gramaje == \"\":\n",
        "        return texto\n",
        "    # Busca el gramaje y lo elimina (con o sin espacio antes)\n",
        "    return re.sub(r'\\s*' + re.escape(gramaje) + r'\\b', '', texto)\n",
        "\n",
        "anonima_df_F['Producto'] = [\n",
        "    quitar_gramaje(prod, gram) for prod, gram in zip(anonima_df_F['Producto'], anonima_df_F['Gramaje'])\n",
        "]\n",
        "\n",
        "#Transformacion para dejar a la columna precios con formato Float\n",
        "def limpiar_precio(precio):\n",
        "    if pd.isna(precio):\n",
        "        return None\n",
        "    precio = str(precio)\n",
        "    precio = precio.replace('$', '').replace('.', '').replace(',', '.').strip()\n",
        "    try:\n",
        "        return float(precio)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "anonima_df_F['Precio'] = anonima_df_F['Precio'].apply(limpiar_precio)\n",
        "\n",
        "#Elimino columna Página\n",
        "anonima_df_F.drop(columns=[\"Página\"], inplace=True)\n",
        "\n",
        "#Reordenar las columnas\n",
        "nuevo_orden = ['Busqueda', 'Producto', 'Gramaje', 'Precio', 'Supermercado']\n",
        "anonima_df_F = anonima_df_F[nuevo_orden]\n",
        "\n",
        "# Guardar el resultado a un nuevo archivo Excel\n",
        "anonima_df_F.to_excel(r'C:\\Users\\Cristian\\Documents\\productos_LaAnonimaFinal.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "7DeUiZGHe82i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modificaciones a la Cooperativa Obrera**"
      ],
      "metadata": {
        "id": "N_Draxpjgb8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cargar excel usando doble barra invertida\n",
        "df = pd.read_excel(\"C:\\\\Users\\\\Cristian\\\\Documents\\\\productos_LaCoope.xlsx\")\n",
        "\n",
        "# Función para limpiar caracteres especiales de un string\n",
        "def limpiar_especiales(texto):\n",
        "    if pd.isna(texto):\n",
        "        return texto\n",
        "    return unidecode(str(texto))\n",
        "\n",
        "# Limpiar acentos y caracteres especiales en la columna 'Producto'\n",
        "df[\"Producto\"] = df[\"Producto\"].apply(lambda x: unidecode(str(x)) if pd.notna(x) else x)\n",
        "\n",
        "# Definir una función para extraer el gramaje (número + unidad)\n",
        "def extraer_gramaje(texto):\n",
        "    # Busca patrones como '500 Gr', '1 Kg', '250 Ml', etc.\n",
        "    patron = r'(\\d+[\\.,]?\\d*\\s?(grs|kg|ml|l|g|gr))'\n",
        "    match = re.search(patron, texto, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(0).strip()\n",
        "    return None\n",
        "\n",
        "# Aplicar la función para crear la columna de Gramaje\n",
        "df[\"Gramaje\"] = df[\"Producto\"].apply(extraer_gramaje)\n",
        "\n",
        "# Eliminar el gramaje del nombre del producto\n",
        "def limpiar_producto(texto):\n",
        "    gramaje = extraer_gramaje(texto)\n",
        "    if gramaje:\n",
        "        return texto.replace(gramaje, \"\").replace(\"  \", \" \").strip()\n",
        "    return texto\n",
        "\n",
        "df[\"Producto\"] = df[\"Producto\"].apply(limpiar_producto)\n",
        "\n",
        "# Lista de palabras clave\n",
        "palabras = [\n",
        "    \"Acondicionador\", \"Alfajor\", \"Aceites\", \"Aceite\", \"Shampoo\", \"Margarina\",\n",
        "    \"Mascarilla\", \"Crema\", \"Protector Labial\", \"Papas Fritas\", \"Paño\", \"Pañales\",\n",
        "    \"Pancetta\", \"Panzottis\", \"Hamburguesas\", \"Hamburguesa\", \"Rallado\", \"Panchos\",\n",
        "    \"Tomate\", \"Licor\", \"Crema\", \"Pañuelos\", \"Batidor\", \"Panceta\", \"Edulcorante\", \"Cerá\", \"Mate De \", \"Autocebante\", \"Bombilla\", \"PaNAles\",\n",
        "    \"PaNO\", \"PaNUelos\", \"Superpancho\", \"CerAMica\", \"Minnie\"\n",
        "]\n",
        "\n",
        "# Unir las palabras con | para usar en regex (para búsqueda OR)\n",
        "patron = r'|'.join([re.escape(palabra) for palabra in palabras])\n",
        "\n",
        "# Filtrar filas donde 'Producto' contiene cualquiera de las palabras (ignora mayúsculas/minúsculas)\n",
        "filtro = df[\"Producto\"].str.contains(patron, case=False, na=False)\n",
        "\n",
        "# Quedarse SOLO con los registros que NO cumplen el filtro (los que NO tienen esas palabras)\n",
        "df_filtrado = df[~filtro]\n",
        "\n",
        "#Cambio Formato de Precio de String a Float\n",
        "def limpiar_precio(precio):\n",
        "    if pd.isna(precio):\n",
        "        return None\n",
        "    precio = str(precio)\n",
        "    precio = precio.replace('$', '').replace('.', '').replace(',', '.').strip()\n",
        "    try:\n",
        "        return float(precio)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "df_filtrado['Precio'] = df_filtrado['Precio'].apply(limpiar_precio)\n",
        "nuevo_orden = ['Busqueda', 'Producto', 'Gramaje', 'Precio', 'Supermercado']\n",
        "df_filtrado = df_filtrado[nuevo_orden]\n",
        "\n",
        "# Guardar el resultado a un nuevo archivo Excel\n",
        "df_filtrado.to_excel(r'C:\\Users\\Cristian\\Documents\\productos_LaCoope2.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "e9X-1O_vlen0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Union de ambos supermercados que pertenecen a Buenos Aires en un mismo dataframe y excel"
      ],
      "metadata": {
        "id": "S5iXQtVCm10u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargo ambos excels\n",
        "dfAnonima = pd.read_excel(\"C:\\\\Users\\\\Cristian\\\\Desktop\\\\Archivos_Finales\\\\productos_LaAnonimaF.xlsx\")\n",
        "dfCoope = pd.read_excel(\"C:\\\\Users\\\\Cristian\\\\Desktop\\\\Archivos_Finales\\\\productos_LaCoope2.xlsx\")\n",
        "\n",
        "# Unilos (uno debajo del otro)al tener las mismas columnas no hay problema\n",
        "df_BsAs = pd.concat([dfAnonima, dfCoope], ignore_index=True)\n",
        "\n",
        "# Listo, ahora df_unido tiene todos los datos juntos\n",
        "print(df_BsAs.head())"
      ],
      "metadata": {
        "id": "d8WQSNb8nIb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le realizo una pequeña transformacion de datos"
      ],
      "metadata": {
        "id": "9KG_Kk73m-ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "df_BsAs['Producto'] = df_BsAs['Producto'].str.replace(\n",
        "    r'\\bx\\.|\\bx\\b|x\\s|\\sx\\s|\\bx', '', regex=True, case=False\n",
        ").str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "def limpiar_producto(row):\n",
        "    texto = row['Producto']\n",
        "    gramaje = str(row['Gramaje']).strip() if pd.notnull(row['Gramaje']) else ''\n",
        "\n",
        "    # 1. Eliminar gramaje literal si existe\n",
        "    if gramaje and gramaje.lower() != 'nan':\n",
        "        texto = texto.replace(gramaje, '')\n",
        "\n",
        "    # 2. Eliminar x, x., x,, x , kg., un., kg, un (con espacios, punto, coma)\n",
        "    patron = r'\\b(x[\\.,]?\\s*|kg\\.?\\s*|un\\.?\\s*)\\b'\n",
        "    texto = re.sub(patron, '', texto, flags=re.IGNORECASE)\n",
        "\n",
        "    # 3. Eliminar si queda solo número(s) y punto o solo punto al final o al principio\n",
        "    texto = re.sub(r'^\\d+\\.\\s*', '', texto)    # al principio\n",
        "    texto = re.sub(r'\\d+\\.\\s*$', '', texto)    # al final\n",
        "    texto = re.sub(r'^\\.\\s*', '', texto)       # punto al principio\n",
        "    texto = re.sub(r'\\.\\s*$', '', texto)       # punto al final\n",
        "\n",
        "    # 4. Quitar dobles espacios y espacios extremos\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "    return texto\n",
        "\n",
        "df_BsAs['Producto'] = df_BsAs.apply(limpiar_producto, axis=1)\n",
        "\n",
        "def eliminar_numero_final(texto):\n",
        "    # Elimina cualquier número (con o sin espacio antes) al final de la cadena\n",
        "    return re.sub(r'\\s*\\d+\\s*$', '', texto).strip()\n",
        "\n",
        "# Si tu DataFrame se llama df_unido:\n",
        "df_BsAs['Producto'] = df_BsAs.apply(limpiar_producto, axis=1)\n",
        "df_BsAs[['Cantidad', 'Unidad']] = df_BsAs['Gramaje'].str.extract(r'(\\d+(?:[\\.,]\\d+)?)([a-zA-Z]+)', expand=True)\n",
        "\n",
        "# Convertir cantidad a float (maneja decimales con coma o punto)\n",
        "df_BsAs['Cantidad'] = df_BsAs['Cantidad'].str.replace(',', '.').astype(float)\n",
        "\n",
        "# Eliminar la columna \"Gramaje\"\n",
        "df_BsAs = df_BsAs.drop(columns=['Gramaje'])\n",
        "\n",
        "# Reordenar columnas (opcional)\n",
        "df_BsAs = df_BsAs[['Busqueda', 'Producto', 'Cantidad', 'Unidad', 'Precio', 'Supermercado']]\n",
        "\n",
        "# Guardar el resultado a un nuevo archivo Excel\n",
        "df_BsAs.to_excel(r'C:\\Users\\Cristian\\Desktop\\Archivos_Finales\\productos_BsAs.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "qvL5M8lnnhm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La transformacion de Datos de los supermercados Coto, Disco, Carrefour y la correspondiente transformacion final de datos se realizó en R.\n",
        "\n",
        "Con la limpieza de datos se buscó extraer cantidad, unidad de medida y precio con descuento (para los casos que aplicaba), al mismo tiempo que filtrar todos los productos que no nos servirían para el análisis. Acá fue donde nos dimos cuenta de la dificultad de la limpieza post-scraping, ya que para hacerlo de una manera eficiente hay que tener un gran conocimiento de la base de datos con la que se trabaja. En vez de hacer la extracción y la limpieza separados, nos resultó más conveniente hacer un ida y vuelta entre los dos y chequear cómo iba quedando el output. La creación de variables nos ayudó a distinguir patrones que permitieron pensar nuevas reglas para eliminar los datos no deseados.\n",
        "\n",
        "A la hora de considerar el escalado de proyecto, estimamos que esto es algo que siempre va a estar presente, porque ya sea por nuevos productos o por el formato de otros supermercados siempre se va a tener que repensar las reglas.\n",
        "\n",
        "Cabe mencionar que en este trabajo decidimos convertir todo a kilogramos para que la información sea comparable, utilizando en algunos casos supuestos sobre algunos productos (ej: todas las capsulas de café son 6 gr). De todas formas, sentimos que esta gran generalización le quita un poco de especificidad al estudio, mostrando productos en dimensiones en los que no se los ve normalmente (no es lo mismo hablar de un kilo de yerba que de un kilo de te) y omitiendo hallazgos que se podrían apreciar mejor examinándolos particularmente.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lHFFOymXqq9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limpieza de datos\n",
        "\n",
        "# ===== CARGAR LIBRERÍAS =====\n",
        "library(readxl)     # leer Excel\n",
        "library(dplyr)      # manipulación de datos\n",
        "library(stringr)    # manejo de strings\n",
        "library(janitor)    # limpieza general\n",
        "library(tidyr)      # para separate()\n",
        "library(ggplot2)    # visualización\n",
        "library(writexl)\n",
        "library(purrr)\n",
        "\n",
        "# ===== CARGAR DATOS =====\n",
        "datos_raw <- read_excel(\"productos.xlsx\")\n",
        "\n",
        "# ===== LIMPIEZA INICIAL =====\n",
        "# Limpiar nombres de columnas\n",
        "datos <- datos_raw %>%\n",
        "  janitor::clean_names()\n",
        "\n",
        "# Eliminar filas sin datos\n",
        "datos <- datos %>%\n",
        "  filter(precio != \"N/A\",\n",
        "         producto != \"N/A\") %>%\n",
        "  drop_na()\n",
        "\n",
        "# Función para extraer información del producto\n",
        "datos <- datos %>%\n",
        "  mutate(\n",
        "    # Limpiar texto\n",
        "    producto_clean = producto %>%\n",
        "      str_to_lower() %>%                    # todo a minúsculas\n",
        "      str_remove_all(\"[®]\") %>%\n",
        "      str_replace_all(\"á\", \"a\") %>%         # quitar tildes\n",
        "      str_replace_all(\"é\", \"e\") %>%\n",
        "      str_replace_all(\"í\", \"i\") %>%\n",
        "      str_replace_all(\"ó\", \"o\") %>%\n",
        "      str_replace_all(\"ú\", \"u\") %>%\n",
        "      str_replace_all(\"ñ\", \"n\") %>%\n",
        "      str_trim() %>%                        # espacios al inicio/final\n",
        "      str_squish(),                         # espacios múltiples\n",
        "\n",
        "    # Extraer cantidad y unidad\n",
        "    # Primero: buscar al final\n",
        "    cantidad_final = str_extract(producto_clean, \"\\\\d+[.,]?\\\\d*\\\\s*[a-zA-Z.]+$\"),\n",
        "\n",
        "    # Segundo: buscar la última aparición si no hay al final\n",
        "    todas_cantidades = str_extract_all(producto_clean, \"\\\\d+[.,]?\\\\d*\\\\s*[a-zA-Z.]+\"),\n",
        "    ultima_cantidad = map_chr(todas_cantidades, ~ifelse(length(.x) > 0, .x[length(.x)], NA_character_)),\n",
        "\n",
        "    # Tercero: cualquier aparición como último recurso\n",
        "    primera_cantidad = str_extract(producto_clean, \"\\\\d+[.,]?\\\\d*\\\\s*[a-zA-Z.]+\"),\n",
        "\n",
        "    # Elegir en orden de prioridad\n",
        "    cantidad_unidad = case_when(\n",
        "      !is.na(cantidad_final) ~ cantidad_final,\n",
        "      !is.na(ultima_cantidad) ~ ultima_cantidad,\n",
        "      !is.na(primera_cantidad) ~ primera_cantidad,\n",
        "      TRUE ~ NA_character_\n",
        "    ),\n",
        "    # Limpiar variables auxiliares\n",
        "    cantidad_unidad = ifelse(cantidad_unidad == \"NA\", NA_character_, cantidad_unidad)\n",
        "  ) %>%\n",
        "  select(-cantidad_final, -todas_cantidades, -ultima_cantidad, -primera_cantidad) %>%\n",
        "  mutate(\n",
        "    cantidad = str_extract(cantidad_unidad, \"\\\\d+[.,]?\\\\d*\"),\n",
        "    unidad = str_extract(cantidad_unidad, \"[a-zA-Z]+\"),\n",
        "    cantidad_num = as.numeric(str_replace(cantidad, \",\", \".\"))\n",
        "  )\n",
        "\n",
        "# Estandarizacion de unidad\n",
        "datos <- datos %>%\n",
        "  mutate(\n",
        "      unidad_std = case_when(\n",
        "      str_detect(producto_clean, \"capsula|capsulas|nespresso|dolce gusto\") ~ \"capsulas\",\n",
        "      unidad %in% c(\"u\", \"un\", \"uni\", \"unidades\") &\n",
        "        str_detect(producto_clean, \"\\\\bte\\\\b|mate cocido\") ~ \"saquitos\",\n",
        "      unidad %in% c(\"gr\", \"g\", \"grs\", \"grm\") ~ \"g\",\n",
        "      unidad %in% c(\"k\", \"kg\", \"kgm\") ~ \"kg\",\n",
        "      unidad %in% c(\"saq\", \"saquitos\",\"s\",\"sob\", \"sobres\") ~ \"saquitos\",\n",
        "      unidad %in% c(\"l\", \"lt\", \"lts\", \"ltr\") ~ \"l\",\n",
        "      unidad %in% c(\"ml\") ~ \"ml\",\n",
        "      unidad %in% c(\"u\", \"un\", \"uni\") ~ \"unidades\",\n",
        "      TRUE ~ NA_character_\n",
        "    )\n",
        "  )\n",
        "\n",
        "# Limpieza específica para cápsulas\n",
        "datos <- datos %>%\n",
        "  mutate(\n",
        "    # Para cápsulas: si cantidad_num > 20 o es NA, cambiar a 10\n",
        "    cantidad_num = case_when(\n",
        "      unidad_std == \"capsulas\" & (cantidad_num > 20 | is.na(cantidad_num)) ~ 10,\n",
        "      TRUE ~ cantidad_num\n",
        "    )\n",
        "  ) %>%\n",
        "  # Eliminar filas de cápsulas con cantidad < 6\n",
        "  filter(\n",
        "    !(unidad_std == \"capsulas\" & cantidad_num < 6),      # cápsulas < 6\n",
        "    !(unidad_std == \"saquitos\" & (cantidad_num < 10 | cantidad_num > 100)),  # saquitos fuera de rango\n",
        "    !(unidad_std %in% c(\"ml\", \"l\") & busqueda != \"yogur\"),          # ml y l que no sean yogur\n",
        "    !(unidad_std == \"kg\" & busqueda %in% c(\"mermelada\", \"te\")),  # kg de mermelada y té\n",
        "    !is.na(unidad_std), # eliminar filas sin unidad_std\n",
        "    unidad_std != \"unidades\",  # eliminar unidades\n",
        "    !str_detect(producto_clean, \"rebozador|pan rallado|horno|jugo|hamburguesa|pancho|jabon|galletitas|smart|aromatizante|pochoclo|enduido|alfajor|torta|protector|desodorante|coffe|coffee\")  # palabras no deseadas\n",
        "  )\n",
        "\n",
        "datos <- datos %>%\n",
        "  mutate(\n",
        "    # Convertir todo a kg para estandarizar\n",
        "    cantidad_num = case_when(\n",
        "      unidad_std == \"capsulas\" ~ (cantidad_num * 6) / 1000,    # 6g por cápsula → kg\n",
        "      unidad_std == \"saquitos\" ~ (cantidad_num * 2) / 1000,    # 2g por saquito → kg\n",
        "      unidad_std == \"kg\" ~ cantidad_num,                        # ya está en kg\n",
        "      unidad_std == \"g\" ~ cantidad_num / 1000,                 # g a kg\n",
        "      unidad_std == \"ml\" ~ cantidad_num / 1000,                # ml a kg (1ml ≈ 1g)\n",
        "      unidad_std == \"l\" ~ cantidad_num,                        # l a kg (1l ≈ 1kg)\n",
        "      TRUE ~ NA_real_\n",
        "    ),\n",
        "\n",
        "    # Actualizar unidad_std y cantidad_num para que sea consistente\n",
        "    unidad_std = case_when(\n",
        "      unidad_std %in% c(\"capsulas\", \"saquitos\", \"kg\", \"g\", \"ml\", \"l\") ~ \"kg\",\n",
        "      TRUE ~ unidad_std  # mantener otras unidades como están\n",
        "    )\n",
        "  )\n",
        "\n",
        "datos <- datos %>%\n",
        "  filter(\n",
        "    # Si la búsqueda es \"te\", entonces el producto tiene que contener \"te\" o \"infusion\"\n",
        "    !(busqueda == \"te\" & !str_detect(producto_clean, \"\\\\bte\\\\b|infusion\"))\n",
        "  )\n",
        "\n",
        "# Limpieza de precios\n",
        "datos <- datos %>%\n",
        "  mutate(\n",
        "    # Extraer descuento si existe\n",
        "    descuento = str_extract(precio, \"\\\\d+%\"),\n",
        "    descuento_num = as.numeric(str_remove(descuento, \"%\")),\n",
        "\n",
        "    # Limpiar precio de forma más flexible\n",
        "    precio_limpio = precio %>%\n",
        "      str_remove(\"\\\\s*c/u\\\\s*\") %>%                    # quitar \"c/u\"\n",
        "      str_remove(\"\\\\s*-\\\\s*\\\\d+%.*\") %>%               # quitar descuento y lo que sigue\n",
        "      str_extract(\"\\\\$?[\\\\d.,]+\") %>%                  # extraer números con $ opcional\n",
        "      str_remove(\"\\\\$\") %>%                            # quitar $\n",
        "      str_replace_all(\"\\\\.\", \"\") %>%                   # quitar puntos de miles\n",
        "      str_replace(\",\", \".\") %>%                        # cambiar coma decimal por punto\n",
        "      str_trim(),                                      # quitar espacios\n",
        "\n",
        "    # Convertir a numérico\n",
        "    precio_base = as.numeric(precio_limpio),\n",
        "\n",
        "    # Calcular precio final con descuento si existe\n",
        "    precio_final = case_when(\n",
        "      !is.na(descuento_num) ~ precio_base * (1 - descuento_num/100),\n",
        "      TRUE ~ precio_base\n",
        "    )\n",
        "  )\n",
        "\n",
        "# ===== CREAR DATASET FINAL =====\n",
        "datos_finales <- datos %>%\n",
        "  select(\n",
        "    busqueda,\n",
        "    producto = producto_clean,\n",
        "    cantidad = cantidad_num,\n",
        "    unidad = unidad_std,\n",
        "    precio = precio_final,\n",
        "    supermercado = super\n",
        "  ) %>%\n",
        "  # Opcional: reordenar por supermercado y búsqueda\n",
        "  arrange(supermercado, busqueda, producto)\n",
        "\n",
        "# ===== CARGAR DATOS BS AS =====\n",
        "datos_ba_raw <- read_excel(\"productos_BsAs.xlsx\")\n",
        "\n",
        "# ===== LIMPIEZA INICIAL =====\n",
        "# Limpiar nombres de columnas\n",
        "datos_ba <- datos_ba_raw %>%\n",
        "  janitor::clean_names()\n",
        "\n",
        "# Eliminar filas sin datos\n",
        "datos_ba <- datos_ba %>%\n",
        "  filter(precio != \"N/A\",\n",
        "         producto != \"N/A\") %>%\n",
        "  drop_na()\n",
        "\n",
        "# Función para extraer información del producto\n",
        "datos_ba <- datos_ba %>%\n",
        "  mutate(\n",
        "    # Limpiar texto\n",
        "    producto_clean = producto %>%\n",
        "      str_to_lower() %>%                    # todo a minúsculas\n",
        "      str_remove_all(\"[®]\") %>%\n",
        "      str_replace_all(\"á\", \"a\") %>%         # quitar tildes\n",
        "      str_replace_all(\"é\", \"e\") %>%\n",
        "      str_replace_all(\"í\", \"i\") %>%\n",
        "      str_replace_all(\"ó\", \"o\") %>%\n",
        "      str_replace_all(\"ú\", \"u\") %>%\n",
        "      str_replace_all(\"ñ\", \"n\") %>%\n",
        "      str_trim() %>%                        # espacios al inicio/final\n",
        "      str_squish(),\n",
        "    cantidad = str_extract(gramaje, \"\\\\d+[.,]?\\\\d*\"),\n",
        "    unidad = str_extract(gramaje, \"[a-zA-Z]+\"),\n",
        "    cantidad_num = as.numeric(str_replace(cantidad, \",\", \".\"))\n",
        "  )\n",
        "\n",
        "datos_ba <- datos_ba %>%\n",
        "  mutate(\n",
        "    unidad_std = case_when(\n",
        "      str_detect(producto_clean, \"capsula|capsulas|nespresso|dolce gusto\") ~ \"capsulas\",\n",
        "      str_detect(producto_clean, \"saquitos\") ~ \"saquitos\",\n",
        "      unidad %in% c(\"u\", \"un\", \"uni\", \"unidades\") &\n",
        "        str_detect(producto_clean, \"\\\\bte\\\\b|mate cocido\") ~ \"saquitos\",\n",
        "      unidad %in% c(\"gr\", \"g\", \"grs\", \"grm\") ~ \"g\",\n",
        "      unidad %in% c(\"k\", \"kg\", \"kgm\") ~ \"kg\",\n",
        "      unidad %in% c(\"saq\", \"saquitos\",\"s\",\"sob\", \"sobres\") ~ \"saquitos\",\n",
        "      unidad %in% c(\"l\", \"lt\", \"lts\", \"ltr\") ~ \"l\",\n",
        "      unidad %in% c(\"ml\") ~ \"ml\",\n",
        "      unidad %in% c(\"u\", \"un\", \"uni\") ~ \"unidades\",\n",
        "      TRUE ~ NA_character_\n",
        "    )\n",
        "  )\n",
        "\n",
        "# Limpieza específica para cápsulas\n",
        "datos_ba <- datos_ba %>%\n",
        "  mutate(\n",
        "    # Para cápsulas: si cantidad_num > 20 o es NA, cambiar a 10\n",
        "    cantidad_num = case_when(\n",
        "      unidad_std == \"capsulas\" & (cantidad_num > 20 | is.na(cantidad_num)) ~ 10,\n",
        "      TRUE ~ cantidad_num\n",
        "    )\n",
        "  ) %>%\n",
        "  # Eliminar filas de cápsulas con cantidad < 6\n",
        "  filter(\n",
        "    !(unidad_std == \"capsulas\" & cantidad_num < 6),      # cápsulas < 6\n",
        "    !(unidad_std == \"saquitos\" & (cantidad_num < 10 | cantidad_num > 100)),  # saquitos fuera de rango\n",
        "    !(unidad_std %in% c(\"ml\", \"l\") & busqueda != \"yogur\"),          # ml y l que no sean yogur\n",
        "    !(unidad_std == \"kg\" & busqueda %in% c(\"mermelada\", \"te\")),  # kg de mermelada y té\n",
        "    !is.na(unidad_std), # eliminar filas sin unidad_std\n",
        "    unidad_std != \"unidades\",  # eliminar unidades\n",
        "    !str_detect(producto_clean, \"rebozador|pan rallado|horno|jugo|hamburguesa|pancho|jabon|galletitas|smart|aromatizante|pochoclo|enduido|alfajor|torta|protector|desodorante|coffe|coffee\")  # palabras no deseadas\n",
        "  )\n",
        "\n",
        "datos_ba <- datos_ba %>%\n",
        "  mutate(\n",
        "    # Convertir todo a kg para estandarizar\n",
        "    cantidad_num = case_when(\n",
        "      unidad_std == \"capsulas\" ~ (cantidad_num * 6) / 1000,    # 6g por cápsula → kg\n",
        "      unidad_std == \"saquitos\" ~ (cantidad_num * 2) / 1000,    # 2g por saquito → kg\n",
        "      unidad_std == \"kg\" ~ cantidad_num,                        # ya está en kg\n",
        "      unidad_std == \"g\" ~ cantidad_num / 1000,                 # g a kg\n",
        "      TRUE ~ NA_real_\n",
        "    ),\n",
        "\n",
        "    # Actualizar unidad_std y cantidad_num para que sea consistente\n",
        "    unidad_std = case_when(\n",
        "      unidad_std %in% c(\"capsulas\", \"saquitos\", \"kg\", \"g\") ~ \"kg\",\n",
        "      TRUE ~ unidad_std  # mantener otras unidades como están\n",
        "    )\n",
        "  )\n",
        "\n",
        "# ===== CREAR DATASET FINAL =====\n",
        "datos_ba_finales <- datos_ba %>%\n",
        "  select(\n",
        "    busqueda,\n",
        "    producto = producto_clean,\n",
        "    cantidad = cantidad_num,\n",
        "    unidad = unidad_std,\n",
        "    precio,\n",
        "    supermercado\n",
        "  ) %>%\n",
        "  # Opcional: reordenar por supermercado y búsqueda\n",
        "  arrange(supermercado, busqueda, producto)\n",
        "\n",
        "# Unir los dos datasets\n",
        "datos_completos <- bind_rows(datos_finales, datos_ba_finales)\n",
        "\n",
        "# Ultima limpieza\n",
        "datos_completos <- datos_completos %>%\n",
        "  mutate(\n",
        "    busqueda = ifelse(busqueda == \"queso\", \"queso crema\", busqueda)\n",
        "  )\n",
        "\n",
        "datos_completos <- datos_completos %>%\n",
        "  filter(precio != 0)\n",
        "\n",
        "# Exportar Excel y CSV\n",
        "library(writexl)\n",
        "write_xlsx(datos_completos, \"datos_limpios.xlsx\")\n",
        "write_csv(datos_completos, \"productos_completos.csv\")\n"
      ],
      "metadata": {
        "id": "JxTgwSIbq13X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tablero Dinámico\n",
        "Optamos por el uso del tablero dinámico para visualizar la información dado que tiene una gran facilidad de ajuste a las preferencias del consumidor. El filtro de supermercados permite sortear el hecho de que todas las personas no tienen la misma posibilidad de acceso a los distintos supermercados (distancia geográfica) y el filtro por categoría permite seleccionar solo los productos en los cuales uno está interesado.\n",
        "Por otro lado, si uno piensa en el escalado a futuro del trabajo, es fácil adaptarlo a la introducción de más data, ya sea por un aumento de observaciones (inclusión de nuevos productos) o de variables (fecha, marca)."
      ],
      "metadata": {
        "id": "sDOXmO8b-cT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones\n",
        "Durante el filtrado, encontramos dificultades porque los productos eran muy variados y presentaban diferentes formas de nombrado y presentación. Sin embargo, logramos generalizar la información para facilitar el análisis. Finalmente, a partir de las preguntas planteadas, construimos gráficos que nos permitieron visualizar nuestras hipótesis. Los resultados muestran que, en general, vivir en la provincia de Buenos Aires resulta más barato que en la capital, ya que el precio promedio de los supermercados es menor. La desventaja es que la variedad de productos es más reducida en la provincia, en comparación con los supermercados que se encuentran en la capital. En conclusión, este trabajo no solo nos permitió comparar precios, sino también entender la importancia de limpiar y preparar los datos para obtener resultados confiables y útiles para la toma de decisiones.\n"
      ],
      "metadata": {
        "id": "g9xhKjGj-o_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limitaciones y trabajo futuro\n",
        "Este trabajo es un primer acercamiento al uso de scraping en páginas web de supermercados por lo que se usó una única extracción de una canasta reducida, pero se puede expandir el análisis agregando más productos/categorías.\n",
        "\n",
        "Igualmente esto impacta en una de las mayores dificultades: la limpieza de datos. La complejidad surge de las discrepancias en la forma de catalogar y las especificidades de cada producto, por lo que hay que tener un gran conocimiento de la base de datos para crear reglas que permitan filtrar elementos no deseados y estandarizar la información.\n",
        "\n",
        "Otra forma de escalar el análisis es realizar una extracción mensual de los datos con el objetivo de agregar una componente temporal al análisis y estudiar los patrones inflacionarios por categoría y/o supermercado."
      ],
      "metadata": {
        "id": "4Il6wApq-1Gi"
      }
    }
  ]
}